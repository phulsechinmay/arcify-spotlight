---
phase: 01-test-infrastructure-setup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - vitest.config.js
  - test/setup.js
  - test/mocks/chrome.js
  - test/example.test.js
  - e2e/setup.js
  - e2e/example.e2e.test.js
  - .github/workflows/test.yml
autonomous: true

must_haves:
  truths:
    - "Developer can run npm test and Vitest executes cleanly"
    - "Developer can run npm run test:e2e and Puppeteer executes cleanly"
    - "Git push triggers CI workflow that runs all tests"
    - "Coverage report is generated showing 0% baseline"
  artifacts:
    - path: "vitest.config.js"
      provides: "Vitest configuration for unit tests"
      contains: "defineConfig"
    - path: "test/setup.js"
      provides: "Test setup with Chrome API mocks"
      min_lines: 5
    - path: "test/mocks/chrome.js"
      provides: "Chrome API mock scaffolding"
      min_lines: 10
    - path: "e2e/setup.js"
      provides: "Puppeteer E2E test setup"
      min_lines: 10
    - path: ".github/workflows/test.yml"
      provides: "CI workflow for automated testing"
      contains: "npm test"
    - path: "package.json"
      provides: "Test scripts"
      contains: "test:e2e"
  key_links:
    - from: "package.json"
      to: "vitest.config.js"
      via: "npm test script"
      pattern: "vitest"
    - from: "package.json"
      to: "e2e/"
      via: "npm run test:e2e script"
      pattern: "test:e2e"
    - from: ".github/workflows/test.yml"
      to: "package.json"
      via: "npm test and npm run test:e2e commands"
      pattern: "npm (run )?test"
---

<objective>
Set up complete test infrastructure for Arcify Spotlight extension

Purpose: Establish the foundation for all testing in v1.01 milestone. This enables fast feedback loops during development and regression prevention for future changes.

Output:
- Vitest configured for unit/integration testing with Chrome API mock scaffolding
- Puppeteer configured for E2E testing of extension functionality
- GitHub Actions CI workflow running tests on every push
- Coverage reporting at 0% baseline (ready for test implementation)
</objective>

<execution_context>
@/Users/phulsechinmay/.claude/get-shit-done/workflows/execute-plan.md
@/Users/phulsechinmay/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@package.json
@vite.config.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configure Vitest for Unit Testing</name>
  <files>
    package.json
    vitest.config.js
    test/setup.js
    test/mocks/chrome.js
    test/example.test.js
  </files>
  <action>
    Install Vitest and coverage dependencies:
    - `npm install -D vitest @vitest/coverage-v8`

    Create vitest.config.js at project root:
    - Use defineConfig from vitest/config
    - Set test.globals to true for describe/it/expect without imports
    - Set test.environment to 'node' (Chrome APIs will be mocked)
    - Set test.include to ['test/**/*.test.js']
    - Set test.setupFiles to ['./test/setup.js']
    - Configure coverage with provider 'v8', reporter ['text', 'html', 'lcov']
    - Set coverage.include to source files: ['shared/**/*.js', '*.js'] (exclude node_modules, dist, test)
    - Set coverage.exclude to ['node_modules/**', 'dist/**', 'dist-dev/**', 'test/**', 'e2e/**', 'vite*.js', 'vite-plugins/**']

    Create test/setup.js:
    - Import and apply Chrome API mocks from './mocks/chrome.js'
    - Set global.chrome = chromeMock

    Create test/mocks/chrome.js:
    - Export a chromeMock object with stub implementations for:
      - chrome.tabs (query, update, create, get - return Promises)
      - chrome.storage.local (get, set - return Promises)
      - chrome.bookmarks (search - return Promise)
      - chrome.history (search - return Promise)
      - chrome.runtime (sendMessage, onMessage.addListener)
      - chrome.tabGroups (get - return Promise)
    - Each method should be a vi.fn() that returns a resolved Promise with empty/default data
    - Include helper to reset all mocks: resetChromeMocks()

    Create test/example.test.js (placeholder to verify setup works):
    - Single test: describe('Test Setup', () => { it('vitest runs', () => { expect(true).toBe(true); }); });

    Update package.json scripts:
    - Add "test": "vitest run"
    - Add "test:watch": "vitest"
    - Add "test:coverage": "vitest run --coverage"
  </action>
  <verify>
    Run `npm test` - should execute vitest and pass with 1 test
    Run `npm run test:coverage` - should show coverage report (0% on source files is expected)
  </verify>
  <done>
    - vitest.config.js exists with proper configuration
    - test/setup.js applies Chrome mocks globally
    - test/mocks/chrome.js provides mock scaffolding for all needed Chrome APIs
    - npm test runs successfully with 1 passing placeholder test
    - Coverage report generates (text output visible, html in coverage/ folder)
  </done>
</task>

<task type="auto">
  <name>Task 2: Configure Puppeteer for E2E Testing</name>
  <files>
    package.json
    e2e/setup.js
    e2e/example.e2e.test.js
  </files>
  <action>
    Install Puppeteer:
    - `npm install -D puppeteer`

    Create e2e/setup.js with extension loading helper:
    - Export async function launchBrowserWithExtension(options = {})
    - Use puppeteer.launch() with:
      - headless: false (required for extensions)
      - args: ['--disable-extensions-except=./dist', '--load-extension=./dist']
      - Spread any additional options passed
    - Return the browser instance
    - Export async function closeBrowser(browser) for cleanup
    - Note: Extension must be built first (npm run build)

    Create e2e/example.e2e.test.js (placeholder):
    - Import setup functions
    - Single test that:
      - Launches browser with extension
      - Opens a new page
      - Navigates to chrome://newtab (or any URL)
      - Verifies page loads (page.title() is not empty or page exists)
      - Closes browser
    - Use try/finally to ensure browser closes even on failure

    Update package.json scripts:
    - Add "test:e2e": "npm run build && node --test e2e/**/*.e2e.test.js"
    - Using Node's built-in test runner for E2E (simpler than vitest for browser tests)
    - The build step ensures dist/ is up to date before testing

    Add .gitignore entries (if not present):
    - coverage/
  </action>
  <verify>
    Run `npm run build` first (ensure dist exists)
    Run `npm run test:e2e` - should launch Chrome with extension, run placeholder test, close browser
  </verify>
  <done>
    - e2e/setup.js exports launchBrowserWithExtension and closeBrowser helpers
    - e2e/example.e2e.test.js runs a placeholder test that opens browser and closes it
    - npm run test:e2e executes successfully (builds first, then runs e2e test)
    - Browser launches visibly during test (headless: false is required for extensions)
  </done>
</task>

<task type="auto">
  <name>Task 3: Create GitHub Actions CI Workflow</name>
  <files>
    .github/workflows/test.yml
  </files>
  <action>
    Create .github/workflows/test.yml:

    ```yaml
    name: Tests

    on:
      push:
        branches: [main]
      pull_request:
        branches: [main]

    jobs:
      test:
        runs-on: ubuntu-latest

        steps:
          - uses: actions/checkout@v4

          - name: Setup Node.js
            uses: actions/setup-node@v4
            with:
              node-version: '20'
              cache: 'npm'

          - name: Install dependencies
            run: npm ci

          - name: Run unit tests with coverage
            run: npm run test:coverage

          - name: Build extension
            run: npm run build

          # E2E tests require display - use xvfb-run for headless environment
          - name: Run E2E tests
            run: xvfb-run --auto-servernum npm run test:e2e
            env:
              DISPLAY: ':99'

          - name: Upload coverage report
            uses: actions/upload-artifact@v4
            with:
              name: coverage-report
              path: coverage/
              retention-days: 7
    ```

    Key points:
    - Runs on push to main and PRs to main
    - Uses Node 20 (LTS)
    - npm ci for reproducible installs
    - Runs unit tests first (fast feedback)
    - Builds extension before E2E
    - Uses xvfb-run for E2E since Puppeteer needs a display
    - Uploads coverage as artifact
  </action>
  <verify>
    Verify .github/workflows/test.yml exists and is valid YAML
    Commit and push to trigger workflow (or verify locally with `act` if available)
  </verify>
  <done>
    - .github/workflows/test.yml exists with correct structure
    - Workflow runs unit tests with coverage
    - Workflow builds extension before E2E tests
    - Workflow runs E2E tests with xvfb-run for display
    - Coverage report uploaded as artifact
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. `npm test` runs Vitest and passes (INFRA-01)
2. `npm run test:e2e` builds and runs Puppeteer tests (INFRA-02)
3. `.github/workflows/test.yml` exists and is valid (INFRA-03)
4. `npm run test:coverage` generates coverage report in coverage/ (INFRA-04)

Run full verification:
```bash
npm test && npm run test:coverage && npm run test:e2e
```
</verification>

<success_criteria>
- INFRA-01: `npm test` executes Vitest and exits with code 0
- INFRA-02: `npm run test:e2e` executes Puppeteer and exits with code 0
- INFRA-03: `.github/workflows/test.yml` is valid YAML with test jobs
- INFRA-04: `coverage/` directory created with HTML and LCOV reports
</success_criteria>

<output>
After completion, create `.planning/phases/01-test-infrastructure-setup/01-01-SUMMARY.md`
</output>
