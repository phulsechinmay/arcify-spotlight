---
phase: 13-audit-coverage-report
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md
autonomous: false

must_haves:
  truths:
    - "A coverage report exists with per-module line, branch, and function metrics for every source file in coverage scope"
    - "Every source module is mapped to its test file(s) with a gap classification (untested, partially tested, well tested)"
    - "A prioritized gap list ranks modules by risk using code complexity, change frequency, and user-facing impact"
    - "The gap list has been presented to the user for review before any test implementation begins"
  artifacts:
    - path: ".planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md"
      provides: "Complete audit report with coverage metrics, source-to-test mapping, gap classifications, and prioritized gap list"
      contains: "## Coverage Metrics"
  key_links:
    - from: "npx vitest run --coverage"
      to: "COVERAGE-REPORT.md"
      via: "V8 coverage data parsed into markdown tables"
      pattern: "Lines|Branches|Functions"
---

<objective>
Run V8 coverage analysis on the entire codebase, map every source module to its test files with gap classification, produce a prioritized gap list ranked by risk, and present it to the user for approval.

Purpose: Give the user a complete, data-driven picture of test coverage so they can make informed decisions about which gaps to fill before any test writing begins.
Output: `.planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md` containing coverage metrics, source-to-test mapping, and prioritized gap list.
</objective>

<execution_context>
@/Users/phulsechinmay/.claude/get-shit-done/workflows/execute-plan.md
@/Users/phulsechinmay/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@vitest.config.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run V8 coverage and produce COVERAGE-REPORT.md</name>
  <files>.planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md</files>
  <action>
    This task runs coverage analysis, maps source modules to test files, classifies gaps, and produces a single comprehensive report.

    **Step 1: Run V8 coverage**

    Run `npx vitest run --coverage` from the project root. This uses the V8 provider already configured in `vitest.config.js`. Capture the text output which includes per-file line, branch, function, and statement percentages.

    The coverage scope (from vitest.config.js) is:
    - `shared/**/*.js` (15 files)
    - `*.js` root-level files (6 files: background.js, bookmark-utils.js, overlay.js, newtab.js, logger.js, utils.js)
    - Excluded: node_modules, dist, dist-dev, test, vite*.js, vite-plugins

    **Step 2: Source-to-test mapping**

    For each source module, identify which test file(s) exercise it. Use this known mapping as a starting point, then verify by grepping test files for imports/references:

    Known test files (15 files, 339 tests):
    - `test/unit/scoring.test.js` -> shared/scoring-constants.js, shared/data-providers/base-data-provider.js
    - `test/unit/selection-manager.test.js` -> shared/selection-manager.js
    - `test/unit/arcify-provider.test.js` -> shared/data-providers/arcify-provider.js
    - `test/unit/arcify-enrichment.test.js` -> shared/data-providers/base-data-provider.js (enrichment pipeline)
    - `test/unit/fuzzy-matching.test.js` -> shared/fuse-search-service.js, shared/data-providers/base-data-provider.js
    - `test/unit/search-engine-debounce.test.js` -> shared/search-engine.js
    - `test/unit/search-engine-cache.test.js` -> shared/search-engine.js
    - `test/unit/action-routing.test.js` -> shared/ui-utilities.js (action text logic)
    - `test/unit/deduplication.test.js` -> shared/data-providers/base-data-provider.js (dedup logic)
    - `test/unit/url-utilities.test.js` -> shared/ui-utilities.js (URL formatting)
    - `test/unit/space-chip-ui.test.js` -> shared/ui-utilities.js (chip rendering)
    - `test/integration/message-passing.test.js` -> shared/message-client.js, background.js
    - `test/integration/activation-flow.test.js` -> overlay.js, newtab.js, background.js
    - `test/integration/regression.test.js` -> integration-level regression tests
    - `test/example.test.js` -> basic sanity test

    For each test file, grep for import/require statements and function references to confirm which source modules it actually touches.

    **Step 3: Classify each source module**

    For each of the 21 source modules, assign a classification:
    - **Well tested**: Has dedicated test file(s) with substantial coverage (>70% lines), core logic paths exercised
    - **Partially tested**: Has some test coverage but significant gaps (30-70% lines), or only tested indirectly through integration tests
    - **Untested**: No test file maps to this module, or coverage is <30% lines

    **Step 4: Prioritize gaps by risk**

    For each module classified as "untested" or "partially tested", assess risk using three factors:

    1. **Code complexity** (HIGH/MEDIUM/LOW): Based on line count, branching logic, number of exported functions. Files >200 lines with complex logic = HIGH.
    2. **Change frequency** (HIGH/MEDIUM/LOW): Based on git log. Files modified in multiple recent phases = HIGH. Use `git log --oneline -- <file> | wc -l` for each file.
    3. **User-facing impact** (HIGH/MEDIUM/LOW): Does this module directly affect what users see or interact with? Data providers and UI utilities = HIGH. Internal helpers = LOW.

    Combine into an overall risk score: HIGH (2+ high factors), MEDIUM (1 high factor or 2+ medium), LOW (all low/medium).

    **Step 5: Write COVERAGE-REPORT.md**

    Write the report to `.planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md` with these sections:

    ```
    # Coverage Audit Report

    ## Summary
    - Total source modules: N
    - Well tested: N
    - Partially tested: N
    - Untested: N
    - Overall line coverage: X%

    ## Coverage Metrics
    [Table with columns: Module | Lines % | Branches % | Functions % | Stmts % | Classification]
    Group by: shared/data-providers/, shared/, root-level

    ## Source-to-Test Mapping
    [Table with columns: Source Module | Test File(s) | Notes]

    ## Prioritized Gap List
    [Table with columns: Priority | Module | Classification | Complexity | Change Freq | User Impact | Risk | Recommended Action]
    Sorted by risk (HIGH first), then by line count descending.

    ## Recommendations
    Brief summary of which gaps to address in Phases 14, 15, 16 and any observations about coverage patterns.
    ```

    IMPORTANT: Do NOT write any test code. This is an audit-only phase. The report informs the user's decisions about what tests to write in subsequent phases.
    IMPORTANT: Use actual numbers from `npx vitest run --coverage` output. Do not estimate or fabricate coverage percentages.
    IMPORTANT: For change frequency, run `git log --oneline -- <file> | wc -l` for each source file individually.
  </action>
  <verify>
    1. File `.planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md` exists
    2. Report contains a "Coverage Metrics" section with a table showing all 21 source modules
    3. Report contains a "Source-to-Test Mapping" section mapping each source to its test file(s)
    4. Report contains a "Prioritized Gap List" section with risk assessments
    5. All coverage percentages come from actual V8 output (not fabricated)
    6. No test files were created or modified
  </verify>
  <done>
    COVERAGE-REPORT.md exists with actual V8 coverage data for all source modules, each module mapped to test files with gap classification, and a prioritized gap list sorted by risk.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete coverage audit report with:
    - V8 coverage metrics (line, branch, function) for every source module
    - Source-to-test file mapping showing which tests cover which modules
    - Gap classification (untested, partially tested, well tested) for each module
    - Prioritized gap list ranked by risk (code complexity x change frequency x user-facing impact)
  </what-built>
  <how-to-verify>
    1. Review the COVERAGE-REPORT.md file at `.planning/phases/13-audit-coverage-report/COVERAGE-REPORT.md`
    2. Check the prioritized gap list -- does the risk ranking make sense given your knowledge of the codebase?
    3. Review the recommendations for Phases 14, 15, 16 -- do they align with your testing priorities?
    4. Decide: approve the gap list as-is, or request changes to priorities/scope before test implementation begins
  </how-to-verify>
  <resume-signal>Type "approved" to proceed with test implementation in Phases 14-16, or describe any changes needed to the gap list priorities</resume-signal>
</task>

</tasks>

<verification>
- COVERAGE-REPORT.md exists and contains actual V8 coverage data (not estimates)
- All 21 source modules appear in the coverage metrics table
- Each source module has a gap classification
- Prioritized gap list is sorted by risk with clear rationale
- No test code was written -- this is audit only
</verification>

<success_criteria>
1. User has reviewed the coverage report and understands the current state of test coverage
2. User has seen the prioritized gap list and either approved it or requested changes
3. The report provides a clear, data-driven basis for planning Phases 14, 15, and 16
</success_criteria>

<output>
After completion, create `.planning/phases/13-audit-coverage-report/13-01-SUMMARY.md`
</output>
